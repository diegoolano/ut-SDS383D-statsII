\documentclass{homework}

\linespread{1.2}

\usepackage[T1]{fontenc}\usepackage{palatino}
\usepackage{amssymb,amsmath,amsthm,mathtools}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[babel]{csquotes}
\usepackage[pdftex]{hyperref}
\usepackage{enumitem}
\usepackage[usenames,dvipsnames,table,xcdraw]{xcolor}

\usepackage{graphicx} 
\usepackage{verbatim} % Commenti in blocco con \begin{comment}
\usepackage{bm}
\usepackage[font={small,it}]{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{array}
\usepackage{enumitem}
\setlist[enumerate]{label*=\arabic*.}
	
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings}

\definecolor{mygraybackground}{gray}{0.95}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

\lstset{language=C++,
backgroundcolor=\color{lbcolor},
    tabsize=3,    
        basicstyle=\scriptsize\ttfamily,
        showstringspaces=false,
        breaklines=true,
        frame=single,
        numbers=left,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color{blue},
        captionpos=b,   
        commentstyle=\color{ForestGreen},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        numberstyle=\small\ttfamily\color{Gray}
}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% Simbolo iid
\newcommand\iid{\stackrel{\mathclap{\normalfont\tiny\mbox{iid}}}{\sim}}
% Simbolo ind
\newcommand\ind{\stackrel{\mathclap{\normalfont\tiny\mbox{ind}}}{\sim}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\sign}{sign}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\title{SDS 383D: Exercise 2}
\author{Diego Garcia-Olano}
\begin{document}
\makeatletter
\begin{titlepage}
	\vspace*{\fill}
	\centering
	{\huge \@title \par}
	\vskip0.5cm
	{\large \@author \par}
	\vskip0.5cm
	{\large \today \par}
	\vspace*{\fill}
\end{titlepage}
\makeatother
\newpage 
\mbox{}
\thispagestyle{empty}
\newpage
\setcounter{page}{1}

\problem{Bayes and the Gaussian linear model}

\par \textbf{A simple Gaussian location model}
\par Take a simple Gaussian model with unknown mean and variance:
\begin{equation} \begin{split}
(yi | \theta,\sigma^2) \sim N(\theta,\sigma^2) , i = 1,\dots,n. 
\end{split}\end{equation}
Let $y$ be the vector of observations $y = (y_1, . . . , y_n)^T$.\\

\par Suppose we place conjugate normal and inverse-gamma priors on $\theta$  and $\sigma^2$, respectively:
$$p(\theta | \sigma^2) \sim N(\mu, \tau^2\sigma^2 )$$
$$\sigma^2 \sim InvGamma(\frac{d}{2}, \frac{\eta}{2} )$$
\par where $\mu, \tau > 0$, $d > 0$ and $\eta > 0$ are fixed scalar hyperparameters.\\

\par *Note a crucial choice here: the error variance $\sigma^2$ appears in the prior for $\theta$. 
\par This affects the interpretation of the hyperparameter $\tau$, \par which is not the prior variance of $\theta$, but rather the prior signal-to-noise ratio.\par This is pretty common thing to do in setting up priors for location parameters: \par to \textit{ scale the prior by the error variance}. There are a few good reasons to do this, \par but historically the primary one has been analytical convenience (as you'll now see). \\

\par \noindent Here's a sensible way to interpret each of these four parameters:
\begin{itemize}
\item $\mu$ is a prior guess for $\theta$.
\item $\tau$ is a prior signal-to-noise ratio \\- that is, how disperse your prior is for $\theta$, relative to the error standard deviation $\sigma$.
\item $d$ is like a \textit{"prior sample size"} for the error variance $\sigma^2$.
\item $\eta$ is like a \textit{"prior sum of squares"} for the error variance $\sigma^2$. \\ More transparently, $\eta / d$ is like a \textbf{"prior guess"} for the error variance $\sigma^2$. It's not exactly the prior mean for $\sigma^2$, but it's close to the prior mean as $d$ gets larger, since the inverse-gamma(a,b) prior has expected value
$$E(\sigma^2) = \frac{b}{a - 1} = \frac{\eta/2}{d /2 - 1} = \frac{\eta}{d - 2}$$
if d is large.  This expression is only valid if d > 2.
\end{itemize}
\par \noindent What is meant by \textit{"prior sample size"} ( $d$ ) and \textit{"prior sum of squares"} ( $\eta$ )? 
\\Remember that \textbf{conjugate priors always resemble the likelihood functions} that they're intended to play nicely with. The \underline{two} relevant quantities in the \underline{likelihood function for $\sigma^2$} are (i) the sample size and (ii) the sums of squares. The prior here is designed to mimic the likelihood function for $\sigma^2$ that you'd get if you had a previous data set with sample size $d$ and sums of squares $\eta$.\\

\par \textit{Precisions are easier than variances}. It's perfectly fine to work with this form of the prior, and it's easier to interpret this way. But it turns out that we can make the algebra a bit cleaner by working with the precisions:  $\omega = \frac{1}{\sigma^2}$ and $\kappa = \frac{1}{\tau^2}$ instead.

$$p( \theta | \omega) \sim N\big(\mu, (\omega\kappa)^{-1} \big)$$
$$\omega \sim Gamma(\frac{d}{2}, \frac{\eta}{2} )$$

This means that the joint prior for $(\theta, \omega)$ has the form:
\begin{equation} \begin{split}
p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big)
\end{split}\end{equation}
This is often called the $normal/gamma$ prior for $(\theta, \omega)$ with parameters $(\mu, \kappa, d, \eta)$, \\ and its equivalent to a normal/inverse-gamma prior for $(\theta, \sigma^2)$. \\ The interpretation of $\kappa$ is like a \textit{prior sample size} for the mean $\theta$ \\ \\
Note: you can obviously write this joint density for $p(\theta | \omega)$ in a way that combines the exponential terms, but this way keeps the bit involving $\theta$ separate, so that you can recognize the normal kernel.  The term "kernel" is heavily overloaded in statistics so see \url{https://en.wikipedia.org/wiki/Kernel_(statistics)#In_Bayesian_statistics}.


\begin{enumerate}[label=(\Alph*)]
\item By construction, we know that the marginal prior distribution $p(\theta)$ is a gamma mixture of normals. Show that this takes the form of a centered, scaled t distribution:
$$p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$$
with center $m$, scale $s$, and degrees of freedom $v$, \\ where you fill in the blank for $m$, $s^2$, and $v$ in terms of the four parameters\\ of the normal-gamma family. * you did a problem like this in exercises 1!\\

\par \noindent By definition, the marginal of $\theta$ is the integral of the joint distribution $p(\theta,\omega)$ with respect to $\omega$ (ie, we integrate out $\omega$ ) so 
\begin{equation} \begin{split}
p(\theta) & = \int p(\theta, \omega) d\omega \\
& \propto \int  \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big)  \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big)\\
& = \int \omega^{\frac{d+1}{2} - 1 } \cdot exp\big(  \frac{\omega\kappa(\theta - \mu)^2 + -\omega\eta}{2}\big) \\
& = \int \omega^{( \frac{d+1}{2} - 1) } \cdot exp\big(  - \omega ( \frac{\kappa(\theta - \mu)^2 + \eta}{2} )\big) \\
\end{split} \end{equation}
This is the kernel for gamma(a,b) for $a = \frac{d + 1}{2}$ and $b = \frac{\kappa(\theta - \mu)^2 + \eta}{2} $\\ As a probability distribution, it must integrate to 1/c where c is a constant. \\
In the case for a gamma distribution the leading constant for the pdf is \\$\frac{b^a}{\Gamma(a)} = c$ thus $1/c  =   \frac{\Gamma(a)}{b^a}$ so we can rewrite (1) as:
\begin{equation} \begin{split}
& = \Gamma(\frac{d+1}{2})( \frac{\kappa(\theta - \mu)^2 + \eta}{2} )^{-\frac{d+1}{2}} \\
& \propto ( \frac{\kappa(\theta - \mu)^2 + \eta}{2} )^{-\frac{d+1}{2}} \\
& = ( \frac{\eta}{2} + \frac{\kappa(\theta - \mu)^2}{2} )^{-\frac{d+1}{2}} \\
& = \big(\frac{\eta}{2} \cdot ( 1 + \frac{\kappa(\theta - \mu)^2}{\eta}) \big)^{-\frac{d+1}{2}} \\
& = \frac{\eta}{2}^{-\frac{d+1}{2}} \cdot ( 1 + \frac{\kappa(\theta - \mu)^2}{\eta})^{-\frac{d+1}{2}} \\
\end{split} \end{equation}
Since we only care about the value of the equation with respect to $\theta$, \\we can treat the first part of the equation as constant
\begin{equation} \begin{split}
& = ( 1 + \frac{\kappa(\theta - \mu)^2}{\eta})^{-\frac{d+1}{2}} \\
& = ( 1 + \frac{d}{d} \cdot \frac{\kappa(\theta - \mu)^2}{\eta})^{-\frac{d+1}{2}} \\
& = ( 1 + \frac{1}{d} \cdot \frac{(\theta - \mu)^2}{\eta d/\kappa})^{-\frac{d+1}{2}} \\
\end{split} \end{equation}
This is close to the form of our solution, $$p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$$
Thus set $v = d$, $m = \mu$ and set $s^2 = \eta d/\kappa$ to get the centered, scaled t-student form.

\item  Assume the normal sampling model in Eq 1 and the normal-gamma prior in Eq 2. \\
Calculate joint posterior density $p(\theta, \omega | \textbf{y})$, up to constant factors not depending on $\omega$ or $\theta$. \\Show that this is also a normal/gamma prior in the same form as above:
$$ p(\theta, \omega | \textbf{y}) \propto \omega^{(d^*+1)/2 - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\}$$
\par  We have normal sampling model ( eq 1 ):  $(\textbf{y} | \theta,\sigma^2) \sim N(\theta,\sigma^2)  $
\par  and also normal-gamma prior ( eq 2 ) :  $p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) $\\ 
Calculate the joint posterior density, 
\begin{equation} \begin{split}
p( \theta, \omega | \textbf{y}) & \propto p( \textbf{y} | \theta, \omega) p(\theta, \omega) \\ 
& = \omega^{n/2}exp\{ -\omega \cdot ( \frac{S_y + n(\bar{y} - \theta)^2}{2} ) \}  \cdot \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) \\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\omega ( \frac{S_y + n(\bar{y} - \theta)^2 + \kappa(\theta - \mu)^2 + \eta}{2}) \} \\
& =  \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2} ( S_y + n\bar{y}^2 + n\theta^2 - 2n\bar{y}\theta + \kappa\theta^2 + \kappa\mu^2 - 2\kappa\theta\mu + \eta ) \} \\
& =  \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2} \big( (n+\kappa)\theta^2 - 2(n\bar{y} + \kappa\mu)\theta + ( S_y + n\bar{y}^2 + \kappa\mu^2 + \eta) \big)\} \\
\end{split} \end{equation}
The term in 2nd part that is multipled by $-\frac{\omega}{2}$ is of the form $ax^2 -2bx + c$ \\so we can complete the square as such:
\begin{equation} \begin{split}
ax^2 -2bx + c & = a [ x^2 - 2(\frac{b}{a})x + \frac{c}{a} ] \\
& =  a [ x^2 - 2(\frac{b}{a})x + (\frac{b}{a})^2 - (\frac{b}{a})^2 + \frac{c}{a} ] \\
& =  a [ ( x - \frac{b}{a})^2 - (\frac{b}{a})^2  + \frac{c}{a} ] \\
& =  a( x - \frac{b}{a})^2 - \frac{b^2}{a} + c
\end{split} \end{equation}
Now plugging in for $a = n + \kappa$, $x = \theta$, $b = n\bar{y} + \kappa\mu$, and c = the final term in (6), we get: \\
\begin{equation} \begin{split}
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n\bar{y} + \kappa\mu)^2}{n + \kappa} + ( S_y + n\bar{y}^2 + \kappa\mu^2 + \eta) \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + (n\bar{y}^2 + \kappa\mu^2) +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n+\kappa}{n+\kappa}(n\bar{y}^2 + \kappa\mu^2) +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{(-n^2\bar{y}^2 - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{(-n^2\bar{y}^2 - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{( - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu + \kappa n\bar{y}^2 + n\kappa\mu^2)}{n + \kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{n\kappa( - \kappa\mu^2 - 2\bar{y}\mu + \bar{y}^2 + \mu^2)}{n + \kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta \big) \}\\
\end{split} \end{equation}
So we have 
$$p( \theta, \omega | \textbf{y})  \propto \omega^{( \frac{n + d + 1}{2} - 1)}  \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2\} \cdot exp\{ -\frac{\omega}{2}(\frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta) \}$$
which has the form of the normal-gamma
$$ p(\theta, \omega | \textbf{y}) \propto \omega^{(d^*+1)/2 - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\}$$
From this form of the posterior, the new updated parameters are 
\begin{itemize}
\item $\mu \rightarrow \mu^* =  \frac{n\bar{y} + \kappa\mu}{n + \kappa}$
\item $\kappa \rightarrow \kappa^* = n + \kappa$  AND $d \rightarrow d^* = n + d$  
\item $\eta \rightarrow \eta^* = \frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta$
\end{itemize}

%commented out
\iffalse
You may notice that my parameterization of the normal-gamma in Equation 2 differs from, say, the one you might find in textbooks or on websites. I've chosen this parameterization in order to make these four updates for the parameters, above, as simple-looking and intuitive as possible.
Tip: this one is a bit of an algebra slog, with a lot of completing the square, collecting common terms, and cancelling positives with negatives. For example, to make the calculations go more easily, you might first show (or recall, from a previous exercise) that the likelihood can be written in the form
$$ p(\textbf{y} | \theta, \omega) \propto \omega^{n/2}exp\{ -\omega \cdot ( \frac{S_y + n(\bar{y} - \theta)^2}{2} ) \} $$
where $S_y = \Sigma_{i=1}^n (y_i - \bar{y} )^2$ is the sum of squares for the \textbf{y} vector.\\
This expresses the likelihood in terms of the two statistics $\bar{y}$ and $S_y$, which you may recall from your math-stat course are sufficient statistics for $(\theta, \sigma^2)$. Take care in ignoring constants here: some term that is constant in $\theta$ may not be constant in $\omega$, and vice versa.
\fi

\item From the joint posterior just derived, what is the conditional posterior distribution $p(\theta | \textbf{y}, \omega)$? \\
- you can read it off directly from the joint distribution, since you took care to set up things so that the joint posterior was in the same form as Equation 2.
\par* normal - gamma (normal sampling model with gamma prior on precision) from Eq 2: 
$$p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) $$
In general $p(\theta, \omega) \propto p(\omega) p(\theta | \omega)$\\
and $p(\omega) = \omega^{\frac{d+1}{2} - 1 } \cdot \big( -\omega \cdot \frac{\eta}{2}\big)$
and $p(\theta | \omega) = exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2}\big) $\\
Thus reading off from B,  $$p(\theta | \textbf{y}, \omega) \propto exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2\}$$
This is the Normal distribution form,  $p(\theta | \textbf{y}, \omega) \sim N(\frac{n\bar{y} + \kappa\mu}{n + \kappa}, -\omega(n + \kappa))$



\item  From the joint posterior calculated in (B), what is the marginal posterior distribution $p(\omega | y)$? \\ 
- Unlike the previous question, where you could just read it off, here you have to integrate over $\theta$. Ignore constants not depending on $\omega$ in calculating this integral.
\begin{equation} \begin{split}
p(\omega | y) & = \int p(\theta,\omega | y) d\theta \\
& \propto \int \omega^{\frac{d^*+1}{2} - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\} d\theta \\
& =   \omega^{\frac{d^*+1}{2} - 1} \cdot exp\{-\omega \cdot \frac{\eta^*}{2}\} \int exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\} d\theta \\
\end{split} \end{equation}
The integral here is a Normal kernel: $N (\mu^*, \omega\kappa^*)$ and as a probability distribution must integrate to $\frac{1}{c}$ , where c is the constant of proportionality for the Normal density, which in this case is $\omega^{1/2}$.  Thus $\frac{1}{c} = \frac{1}{\omega^{1/2}} = \omega^{-1/2}$ 
\begin{equation} \begin{split}
& = \omega^{\frac{d^*+1}{2} - 1} \cdot exp\{-\omega \cdot \frac{\eta^*}{2}\}  \cdot \omega^{-1/2} \\
& = \omega^{\frac{d^*}{2} - 1} \cdot exp\{-\omega \cdot \frac{\eta^*}{2}\}  
\end{split} \end{equation}
This is the kernel for the gamma distribution, $Gamma(\frac{d^*}{2} , \frac{\eta^*}{2})$ as seen in Eq 2.



\item From (C) and (D), we know that the marginal posterior distribution $p(\theta | \textbf{y})$ is a gamma mixture of normals. Show that this takes the form of a centered, scaled t distribution:
$$p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$$
with center $m$, scale $s$, and degrees of freedom $v$ ( fill in the blank for $m$, $s^2$, and $v$ ). 
express the parameters of this t distribution in terms of the four parameters of the normal-gamma posterior for $(\theta, \omega)$. Note: since you've set up the normal-gamma family in this careful conjugate form, this should require no extra work. It's just part (A), except for the prior rather than the posterior.

\begin{equation} \begin{split}
p(\theta) & = \int p(\theta, \omega | \textbf{y}) d\omega \\
& \propto \int  \omega^{\frac{d^*+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa^*(\theta - \mu^*)^2}{2} \big)  \cdot exp\big( -\omega \cdot \frac{\eta^*}{2}\big) d\omega \\
\end{split} \end{equation}

This is the kernel for gamma(a,b) for $a = \frac{d^* + 1}{2}$ and $b = \frac{\kappa^*(\theta - \mu^*)^2 + \eta^*}{2} $\\ As a probability distribution, it must integrate to 1/c where c is a constant, in this case that for the gamma distribution pdf, $c = \frac{b^a}{\Gamma(a)}$ thus $1/c  =   \frac{\Gamma(a)}{b^a}$ so we can rewrite (1) as:

\begin{equation} \begin{split}
& = \Gamma(\frac{d+1}{2})( \frac{\kappa^*(\theta - \mu^*)^2 + \eta^*}{2} )^{-\frac{d^*+1}{2}} \\
& \propto ( \frac{\kappa^*(\theta - \mu^*)^2 + \eta}{2} )^{-\frac{d^*+1}{2}} \\
& = ( \frac{\eta^*}{2} + \frac{\kappa^*(\theta - \mu^*)^2}{2} )^{-\frac{d^*+1}{2}} \\
& = \big(\frac{\eta^*}{2} \cdot ( 1 + \frac{\kappa^*(\theta - \mu^*)^2}{\eta^*}) \big)^{-\frac{d^*+1}{2}} \\
& = \frac{\eta^*}{2}^{-\frac{d^*+1}{2}} \cdot ( 1 + \frac{\kappa^*(\theta - \mu^*)^2}{\eta^*})^{-\frac{d^*+1}{2}} \\
\end{split} \end{equation}
Since we only care about the value of the equation with respect to $\theta$, \\we can treat the first part of the equation as constant
\begin{equation} \begin{split}
& = ( 1 + \frac{\kappa^*(\theta - \mu^*)^2}{\eta^*})^{-\frac{d^*+1}{2}} \\
& = ( 1 + \frac{d^*}{d^*} \cdot \frac{\kappa^*(\theta - \mu^*)^2}{\eta^*})^{-\frac{d^*+1}{2}} \\
& = ( 1 + \frac{1}{d^*} \cdot \frac{(\theta - \mu^*)^2}{\frac{\eta^*}{d^*\kappa^*}})^{-\frac{d^*+1}{2}} \\
\end{split} \end{equation}
This is close to the form of our solution, $p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$\\
Thus set $v = d^*$, $m = \mu^*$ and set $s^2 = \frac{\eta^*}{d^*\kappa^*}$ to get the centered, scaled t-student form.


\item \textit{True or false}: in the limit as the prior parameters [ $\kappa$, $d$, and $\eta$ ] approach zero, \\the priors $p(\theta)$ and $p(\omega)$ are valid probability distributions.
\\  - \textit{a valid probability distribution must integrate to 1 (or something finite, so that it can normalized to integrate to 1) over its domain}.
$$ p(\omega) = \frac{(\frac{\eta}{2})^{\frac{1}{2}}}{\Gamma(\frac{d}{2})} \omega^{\frac{d}{2} - 1} \cdot exp^{-\frac{\eta}{2}\omega}$$
For the $\omega$ factor, in the left part of the RHS, \\as $d, \eta \rightarrow 0$, $\omega^{\frac{d}{2} - 1}$ approaches $\frac{1}{\omega}$ and its scalar $ \frac{(\frac{\eta}{2})^{\frac{1}{2}}}{\Gamma(\frac{d}{2})} $ approaches $\frac{0}{\Gamma(0)} = \frac{0}{\infty}$ which is ill defined, hence $p(\omega)$ is not a valid probability distribution.  

\item \textit{True or false}: in the limit as the prior parameters [ $\kappa$, $d$, and $\eta$ ] approach zero, \\the posteriors $p(\theta | \textbf{y})$ and $p(\omega | \textbf{y})$ are valid probability distributions.

From (E) we have,
$$p(\theta | \textbf{y} ) = ( 1 + \frac{1}{d^*} \cdot \frac{(\theta - \mu^*)^2}{\frac{\eta^*}{d^*\kappa^*}})^{-\frac{d^*+1}{2}} $$
where the posterior parameters are 
\begin{itemize}
\item $\mu \rightarrow \mu^* =  \frac{n\bar{y} + \kappa\mu}{n + \kappa}$
\item $\kappa \rightarrow \kappa^* = n + \kappa$ 
\item $d \rightarrow d^* = n + d$  
\item $\eta \rightarrow \eta^* = \frac{ n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta$
\end{itemize}
As these posteriors $\rightarrow 0$,  
\begin{itemize}
\item $ \mu^* =  \frac{n\bar{y} + \kappa\mu}{n + \kappa} \rightarrow \frac{n\bar{y} + (0)\mu}{n + (0)} = \frac{n\bar{y}}{n} = \bar{y}$
\item $\kappa^* = n + \kappa \rightarrow n + (0) = n$ 
\item $d^* = n + d \rightarrow n + (0) = n$  
\item $\eta^* = \frac{ n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta \rightarrow 0 + S_y + 0 = S_y$
\end{itemize}
Thus plugging into $p(\theta | \textbf{y} )$ above we get
$$ ( 1 + \frac{1}{n} \cdot \frac{(\theta - \bar{y})^2}{\frac{S_y}{n^2}})^{-\frac{n+1}{2}} $$
which is a valid probability distribution with the form of the centered, scaled t distribution, with parameters
\begin{itemize}
\item $ m = \bar{y}$
\item $ v = n$ 
\item $s^2 = \frac{S_y}{n^2}$
\end{itemize}
$s^2$ is what we would expect if we had no prior knowledge of $\theta$
Thus from a philosophical view point, 
our posterior distributions do not have to rely on valid prior probability distributions.  
This is a contentious issue in the debate between frequentists and bayesians.

From (D) we have,
$$p(\omega | \textbf{y}) = \omega^{\frac{d^*}{2} - 1} \cdot exp\{-\omega \cdot \frac{\eta^*}{2}\}  $$
plugging in the posterior variables here we get
$$ p(\omega | \textbf{y}) = \omega^{\frac{n}{2} - 1} \cdot exp\{-\omega \cdot \frac{S_y}{2}\}  $$
which is a valid gamma distribution,  $Gamma(\frac{n}{2}, \frac{S_y}{2})$ \\

\item Your result in (E) implies that a Bayesian credible interval for $\theta$ takes the form 
$$ \theta \in m \pm t^* \cdot s ,$$ where $m$ and $s$ are the posterior center and scale parameters from (F), \\and $t^*$ is the appropriate critical value of the t distribution for your coverage level and degrees of freedom (e.g. it would be 1.96 for a 95\% interval under the normal distribution).\\ \\
\textit{True or false}: In the limit as the prior parameters $\kappa$, $d$, and $\eta$ approach zero, \\the Bayesian credible interval for $\theta$ becomes identical to the classical (frequentist) confidence interval for $\theta$ at the same confidence level.
\end{enumerate}

\problem{The conjugate Gaussian linear model}

Now consider the Gaussian linear model,\\
$( \textbf{y} | B, \sigma^2) \sim N(XB, (\omega\Lambda)^{-1}),$ \\where: 
\begin{itemize}
\item $\textbf{y}$ is an n vector of responses, 
\item $X$ is an $n x p$ matrix of features, 
\item $\omega$ = $1/\sigma^2$ is the error precision, and 
\item $\Lambda$ is some known matrix. 
\end{itemize}
A typical setup would be $\Lambda = I$, the $n x n$ identity matrix, so that the residuals of the model are i.i.d. normal with variance $\sigma^2$. But we'll consider other setups as well, so we'll leave a generic $\Lambda$ matrix in the sampling model for now.  Note that when we write the model this way, we typically assume one of two things: either \par(1) that both the y variable and all the X variables have been centered to have mean zero, so that an intercept is unnecessary; or \par(2) that X has a vector of 1's as its first column, so the first entry in $B$ is actually the intercept.
\\We'll again work in terms of the precision $\omega = \sigma^2$, and consider a normal-gamma prior for $B$:

\begin{equation} \begin{split}
(B | \omega) \sim N(m,(\omega K)^{-1}) 
\end{split}\end{equation}
\begin{equation} \begin{split}
\omega \sim Gamma(d/2, \eta/2) 
\end{split}\end{equation}
Here $K$ is a $p x p$ precision matrix in the multivariate normal prior for $B$, assumed to be known.\\
The items below follow a parallel path to the derivations you did for the Gaussian location model - except for the multivariate case. Don't reinvent the wheel if you don?t have to: you should be relying heavily on your previous results about the multivariate normal distribution.   That is, if you find yourself completing the square over and over again with matrices and vectors, you should stop and revisit your Exercises 1 solutions.\\

\textit{Basics}
\begin{enumerate}[label=(\Alph*)]
\item Derive the conditional posterior $p(B | \textbf{y}, \omega)$.

\item Derive the marginal posterior $p(\omega | \textbf{y})$

\item Putting these together, derive the marginal posterior $p(B |  \textbf{y} )$.

\item Take a look at the data in "gdpgrowth.csv" from the class website, which has macroeconomic variables for several dozen countries. In particular, consider a linear model (with intercept) for a country?s GDP growth rate (GR6096) versus its level of defense spending as a fraction of its GDP (DEF60). \\
Fit the Bayesian linear model to this data set, choosing $\Lambda = I$ and something diagonal and pretty vague for the prior precision matrix $K = diag(\kappa_1, \kappa_2)$. Inspect the fitted line (graphically). Are you happy with the fit? Why or why not?

\end{enumerate}

\textit{A heavy-tailed error model} \\
Now it's time for your first "real" use of the hierarchical modeling formalism to do something cool. Here's the full model you'll be working with:
\begin{equation} \begin{split}
(\textbf{y}  |  B, \omega, \Lambda) \sim N(XB, (\omega \Lambda)^{-1})\\
\Lambda = diag(\lambda_1, . . . , \lambda_n) \\
\lambda_i \stackrel{iid}{\sim} Gamma(h/2, h/2) \\
( B | \omega ) \sim N(m, (\omega K)^{-1} \\
\omega \sim Gamma(d/2, \eta/2 )
\end{split}\end{equation}
where $h$ is a fixed hyper parameter. \\
\begin{enumerate}[label=(\Alph*)]
\item Under this model, what is the implied conditional distribution $p(y_i | X, B, \omega)$? Notice that li has been marginalized out. This should look familiar.

\item What is the conditional posterior distribution $p(\lambda_i | \textbf{y}, B, \omega)$?

\item Combining these results with those from the "Basics" subsection above, code up a Gibbs sampler that repeatedly cycles through sampling the following three sets of conditional distributions.
\begin{itemize}
\item $p(B | \textbf{y},\omega,\Lambda)$ 
\item $p(\omega | \textbf{y}, \Lambda)$
\item $p(\lambda_i | \textbf{y}, B,\omega)$
\end{itemize}
The first two should follow identically from previous results, except that we are explicitly conditioning on $\Lambda$, which is now a random variable rather than a fixed hyperparameter. \\ \\
If you cycle through these conditional posterior draws a few thousand times, you will build up a \textbf{Markov-chain Monte Carlo (MCMC)} sample \\from the joint posterior distribution $p(B, \omega, \Lambda | \textbf{y})$. \\ \\
Now use your Gibbs sampler (with at least a few thousand draws) to fit this model to the GDP growth rate data for an appropriate choice of $h$. Are you happier with the fit? What's going on here (i.e. what makes the model more or less appropriate for the data)?  An interesting plot will be the posterior mean of $1/\lambda_i$ for each country.
\end{enumerate}
\clearpage

\appendix
\chapter{R code}
\label{chap:code}




\end{document}
