\documentclass{homework}

\linespread{1.2}

\usepackage[T1]{fontenc}\usepackage{palatino}
\usepackage{amssymb,amsmath,amsthm,mathtools}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[babel]{csquotes}
\usepackage[pdftex]{hyperref}
\usepackage{enumitem}
\usepackage[usenames,dvipsnames,table,xcdraw]{xcolor}

\usepackage{graphicx} 
\usepackage{verbatim} % Commenti in blocco con \begin{comment}
\usepackage{bm}
\usepackage[font={small,it}]{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{array}
\usepackage{enumitem}
\setlist[enumerate]{label*=\arabic*.}
	
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings}

\definecolor{mygraybackground}{gray}{0.95}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

\lstset{language=C++,
backgroundcolor=\color{lbcolor},
    tabsize=3,    
        basicstyle=\scriptsize\ttfamily,
        showstringspaces=false,
        breaklines=true,
        frame=single,
        numbers=left,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color{blue},
        captionpos=b,   
        commentstyle=\color{ForestGreen},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        numberstyle=\small\ttfamily\color{Gray}
}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% Simbolo iid
\newcommand\iid{\stackrel{\mathclap{\normalfont\tiny\mbox{iid}}}{\sim}}
% Simbolo ind
\newcommand\ind{\stackrel{\mathclap{\normalfont\tiny\mbox{ind}}}{\sim}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\sign}{sign}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
\title{SDS 383D: Exercise 2}
\author{Diego Garcia-Olano}
\begin{document}
\makeatletter
\begin{titlepage}
	\vspace*{\fill}
	\centering
	{\huge \@title \par}
	\vskip0.5cm
	{\large \@author \par}
	\vskip0.5cm
	{\large \today \par}
	\vspace*{\fill}
\end{titlepage}
\makeatother
\newpage 
\mbox{}
\thispagestyle{empty}
\newpage
\setcounter{page}{1}

\problem{Bayes and the Gaussian linear model}

\par \textbf{A simple Gaussian location model}\par Take a simple Gaussian model with unknown mean and variance:
\begin{equation} \begin{split}(yi | \theta,\sigma^2) \sim N(\theta,\sigma^2) , i = 1,\dots,n. 
\end{split}\end{equation}Let $y$ be the vector of observations $y = (y_1, . . . , y_n)^T$.\\
\par Suppose we place conjugate normal and inverse-gamma priors on $\theta$  and $\sigma^2$, respectively:
$$p(\theta | \sigma^2) \sim N(\mu, \tau^2\sigma^2 )$$
$$\sigma^2 \sim Inv-Gamma(\frac{d}{2}, \frac{\eta}{2} )$$\par where $\mu, \tau > 0$, $d > 0$ and $\eta > 0$ are fixed scalar hyperparameters.\\

\par *Note a crucial choice here: the error variance $\sigma^2$ appears in the prior for $\theta$. 
\par This affects the interpretation of the hyperparameter $\tau$, \par which is not the prior variance of $\theta$, but rather the prior signal-to-noise ratio.\par This is pretty common thing to do in setting up priors for location parameters: \par to \textit{ scale the prior by the error variance}. There are a few good reasons to do this, \par but historically the primary one has been analytical convenience (as you'll now see). \\

\par \noindent Here's a sensible way to interpret each of these four parameters:
\begin{itemize}
\item $\mu$ is a prior guess for $\theta$.
\item $\tau$ is a prior signal-to-noise ratio \\- that is, how disperse your prior is for $\theta$, relative to the error standard deviation $\sigma$.\item $d$ is like a \textit{"prior sample size"} for the error variance $\sigma^2$.\item $\eta$ is like a \textit{"prior sum of squares"} for the error variance $\sigma^2$. \\ More transparently, $\eta / d$ is like a \textbf{"prior guess"} for the error variance $\sigma^2$. It's not exactly the prior mean for $\sigma^2$, but it's close to the prior mean as $d$ gets larger, since the inverse-gamma(a,b) prior has expected value$$E(\sigma^2) = \frac{b}{a - 1} = \frac{\eta/2}{d /2 - 1} = \frac{\eta}{d - 2}$$if d is large.  This expression is only valid if d > 2.
\end{itemize}\par \noindent What is meant by \textit{"prior sample size"} ( $d$ ) and \textit{"prior sum of squares"} ( $\eta$ )? 
\\Remember that \textbf{conjugate priors always resemble the likelihood functions} that they're intended to play nicely with. The \underline{two} relevant quantities in the \underline{likelihood function for $\sigma^2$} are (i) the sample size and (ii) the sums of squares. The prior here is designed to mimic the likelihood function for $\sigma^2$ that you'd get if you had a previous data set with sample size $d$ and sums of squares $\eta$.\\
\par \textit{Precisions are easier than variances}. It's perfectly fine to work with this form of the prior, and it's easier to interpret this way. But it turns out that we can make the algebra a bit cleaner by working with the precisions:  $\omega = \frac{1}{\sigma^2}$ and $\kappa = \frac{1}{\tau^2}$ instead.

$$p( \theta | \omega) \sim N\big(\mu, (\omega\kappa)^{-1} \big)$$
$$\omega \sim Gamma(\frac{d}{2}, \frac{\eta}{2} )$$

This means that the joint prior for $(\theta, \omega)$ has the form:
\begin{equation} \begin{split}
p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big)
\end{split}\end{equation}
This is often called the $normal/gamma$ prior for $(\theta, \omega)$ with parameters $(\mu, \kappa, d, \eta)$, \\ and its equivalent to a normal/inverse-gamma prior for $(\theta, \sigma^2)$. \\ The interpretation of $\kappa$ is like a \textit{prior sample size} for the mean $\theta$ \\ \\
Note: you can obviously write this joint density for $p(\theta | \omega)$ in a way that combines the exponential terms, but this way keeps the bit involving $\theta$ separate, so that you can recognize the normal kernel.  The term "kernel" is heavily overloaded in statistics so see \url{https://en.wikipedia.org/wiki/Kernel_(statistics)#In_Bayesian_statistics}.


\begin{enumerate}[label=(\Alph*)]
\item By construction, we know that the marginal prior distribution $p(\theta)$ is a gamma mixture of normals. Show that this takes the form of a centered, scaled t distribution:
$$p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$$
with center $m$, scale $s$, and degrees of freedom $v$, \\ where you fill in the blank for $m$, $s^2$, and $v$ in terms of the four parameters\\ of the normal-gamma family. * you did a problem like this in exercises 1!\\

\par \noindent By definition, the marginal of $\theta$ is the integral of the joint distribution $p(\theta,\omega)$ with respect to $\omega$ (ie, we integrate out $\omega$ ) so 
\begin{equation} \begin{split}
p(\theta) & = \int p(\theta, \omega) d\omega \\
& \propto \int  \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big)  \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big)\\
& = \int \omega^{\frac{d+1}{2} - 1 } \cdot exp\big(  \frac{\omega\kappa(\theta - \mu)^2 + -\omega\eta}{2}\big) \\
& = \int \omega^{( \frac{d+1}{2} - 1) } \cdot exp\big(  - \omega ( \frac{\kappa(\theta - \mu)^2 + \eta}{2} )\big) \\
\end{split} \end{equation}
This is the kernel for gamma(a,b) for $a = \frac{d + 1}{2}$ and $b = \frac{\kappa(\theta - \mu)^2 + n}{2} $\\ As a probability distribution, it must integrate to 1/c where c is a constant. \\
In the case for a gamma distribution the leading constant for the pdf is \\$\frac{b^a}{\Gamma(a)} = c$ thus $1/c  =   \frac{\Gamma(a)}{b^a}$ so we can rewrite (1) as:
\begin{equation} \begin{split}
& = \Gamma(\frac{d+1}{2})( \frac{\kappa(\theta - \mu)^2 + n}{2} )^{-\frac{d+1}{2}} \\
& \propto ( \frac{\kappa(\theta - \mu)^2 + n}{2} )^{-\frac{d+1}{2}} \\
& = ( \frac{n}{2} + \frac{\kappa(\theta - \mu)^2}{2} )^{-\frac{d+1}{2}} \\
& = \big(\frac{n}{2} \cdot ( 1 + \frac{\kappa(\theta - \mu)^2}{n}) \big)^{-\frac{d+1}{2}} \\
& = \frac{n}{2}^{-\frac{d+1}{2}} \cdot ( 1 + \frac{\kappa(\theta - \mu)^2}{n})^{-\frac{d+1}{2}} \\
\end{split} \end{equation}
Since we only care about the value of the equation with respect to $\theta$, \\we can treat the first part of the equation as constant
\begin{equation} \begin{split}
& = ( 1 + \frac{\kappa(\theta - \mu)^2}{n})^{-\frac{d+1}{2}} \\
& = ( 1 + \frac{d}{d} \cdot \frac{\kappa(\theta - \mu)^2}{n})^{-\frac{d+1}{2}} \\
& = ( 1 + \frac{1}{d} \cdot \frac{\kappa(\theta - \mu)^2}{nd/k})^{-\frac{d+1}{2}} \\
\end{split} \end{equation}
This is close to the form of our solution, $$p(\theta) \propto \big( 1 + \frac{1}{v} \cdot \frac{(x-m)^2}{s^2}\big)^{-\frac{v+1}{2}}$$
Thus set $d = v$, $\mu = m$ and set $n = s^2 \cdot v$ to get the centered, scaled t-student form.

\item  Assume the normal sampling model in Eq 1 and the normal-gamma prior in Eq 2. \\
Calculate joint posterior density $p(\theta, \omega | \textbf{y})$, up to constant factors not depending on $\omega$ or $\theta$. \\Show that this is also a normal/gamma prior in the same form as above:
$$ p(\theta, \omega | \textbf{y}) \propto \omega^{(d^*+1)/2 - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\}$$
\par  We have normal sampling model ( eq 1 ):  $(\textbf{y} | \theta,\sigma^2) \sim N(\theta,\sigma^2)  $
\par  and also normal-gamma prior ( eq 2 ) :  $p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) $\\ 
Calculate the joint posterior density, 
\begin{equation} \begin{split}
p( \theta, \omega | \textbf{y}) & \propto p( \textbf{y} | \theta, \omega) p(\theta, \omega) \\ 
& = \omega^{n/2}exp\{ -\omega \cdot ( \frac{S_y + n(\bar{y} - \theta)^2}{2} ) \}  \cdot \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) \\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\omega ( \frac{S_y + n(\bar{y} - \theta)^2 + \kappa(\theta - \mu)^2 + \eta}{2}) \} \\
& =  \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2} ( S_y + n\bar{y}^2 + n\theta^2 - 2n\bar{y}\theta + \kappa\theta^2 + \kappa\mu^2 - 2\kappa\theta\mu + \eta ) \} \\
& =  \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2} \big( (n+\kappa)\theta^2 - 2(n\bar{y} + \kappa\mu)\theta + ( S_y + n\bar{y}^2 + \kappa\mu^2 + \eta) \big)\} \\
\end{split} \end{equation}
The term in 2nd part that is multipled by $-\frac{\omega}{2}$ is of the form $ax^2 -2bx + c$ \\so we can complete the square as such:
\begin{equation} \begin{split}
ax^2 -2bx + c & = a [ x^2 - 2(\frac{b}{a})x + \frac{c}{a} ] \\
& =  a [ x^2 - 2(\frac{b}{a})x + (\frac{b}{a})^2 - (\frac{b}{a})^2 + \frac{c}{a} ] \\
& =  a [ ( x - \frac{b}{a})^2 - (\frac{b}{a})^2  + \frac{c}{a} ] \\
& =  a( x - \frac{b}{a})^2 - \frac{b^2}{a} + c
\end{split} \end{equation}
Now plugging in for $a = n + \kappa$, $x = \theta$, $b = n\bar{y} + \kappa\mu$, and c = the final term in (6), we get: \\
\begin{equation} \begin{split}
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n\bar{y} + \kappa\mu)^2}{n + \kappa} + ( S_y + n\bar{y}^2 + \kappa\mu^2 + \eta) \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + (n\bar{y}^2 + \kappa\mu^2) +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n+\kappa}{n+\kappa}(n\bar{y}^2 + \kappa\mu^2) +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 - \frac{(n^2\bar{y}^2 + \kappa^2\mu^2 + 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{(-n^2\bar{y}^2 - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{(-n^2\bar{y}^2 - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu)}{n + \kappa} + \frac{n^2\bar{y}^2 + \kappa n\bar{y}^2 + n\kappa\mu^2+ \kappa^2\mu^2}{n+\kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{( - \kappa^2\mu^2 - 2n\bar{y}\kappa\mu + \kappa n\bar{y}^2 + n\kappa\mu^2)}{n + \kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{n\kappa( - \kappa\mu^2 - 2\bar{y}\mu + \bar{y}^2 + \mu^2)}{n + \kappa} +  S_y + \eta \big) \}\\
& = \omega^{( \frac{n + d + 1}{2} - 1)} \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2 + \frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta \big) \}\\
\end{split} \end{equation}
So we have 
$$p( \theta, \omega | \textbf{y})  \propto \omega^{( \frac{n + d + 1}{2} - 1)}  \cdot exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2\} \cdot exp\{ -\frac{\omega}{2}(\frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta) \}$$
which has the form of the normal-gamma
$$ p(\theta, \omega | \textbf{y}) \propto \omega^{(d^*+1)/2 - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\}$$
From this form of the posterior, the new updated parameters are 
\begin{itemize}\item $\mu \rightarrow \mu^* =  \frac{n\bar{y} + \kappa\mu}{n + \kappa}$
\item $\kappa \rightarrow \kappa^* = n + \kappa$  AND $d \rightarrow d^* = n + d$  
\item $\eta \rightarrow \eta^* = \frac{-n\kappa^2\mu^2 \cdot n\kappa( \bar{y} - \mu)^2}{n + \kappa} +  S_y + \eta$
\end{itemize}

%commented out
\iffalseYou may notice that my parameterization of the normal-gamma in Equation 2 differs from, say, the one you might find in textbooks or on websites. I've chosen this parameterization in order to make these four updates for the parameters, above, as simple-looking and intuitive as possible.Tip: this one is a bit of an algebra slog, with a lot of completing the square, collecting common terms, and cancelling positives with negatives. For example, to make the calculations go more easily, you might first show (or recall, from a previous exercise) that the likelihood can be written in the form
$$ p(\textbf{y} | \theta, \omega) \propto \omega^{n/2}exp\{ -\omega \cdot ( \frac{S_y + n(\bar{y} - \theta)^2}{2} ) \} $$
where $S_y = \Sigma_{i=1}^n (y_i - \bar{y} )^2$ is the sum of squares for the \textbf{y} vector.\\This expresses the likelihood in terms of the two statistics $\bar{y}$ and $S_y$, which you may recall from your math-stat course are sufficient statistics for $(\theta, \sigma^2)$. Take care in ignoring constants here: some term that is constant in $\theta$ may not be constant in $\omega$, and vice versa.
\fi

\item From the joint posterior just derived, what is the conditional posterior distribution $p(\theta | \textbf{y}, \omega)$? \\
- you can read it off directly from the joint distribution, since you took care to set up things so that the joint posterior was in the same form as Equation 2.
\par* normal - gamma (normal sampling model with gamma prior on precision) from Eq 2: 
$$p(\theta, \omega) \propto \omega^{\frac{d+1}{2} - 1 } \cdot exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2} \big) \cdot exp\big( -\omega \cdot \frac{\eta}{2}\big) $$
In general $p(\theta, \omega) \propto p(\omega) p(\theta | \omega)$\\
and $p(\omega) = \omega^{\frac{d+1}{2} - 1 } \cdot \big( -\omega \cdot \frac{\eta}{2}\big)$
and $p(\theta | \omega) = exp\big( -\omega \frac{\kappa(\theta - \mu)^2}{2}\big) $\\
Thus reading off from B,  $$p(\theta | \textbf{y}, \omega) \propto exp\{ -\frac{\omega}{2}  \big( (n + \kappa)( \theta - \frac{n\bar{y} + \kappa\mu}{n + \kappa})^2\}$$
This is the Normal distribution form,  $p(\theta | \textbf{y}, \omega) \sim N(\frac{n\bar{y} + \kappa\mu}{n + \kappa}, -\omega(n + \kappa))$



\item  From the joint posterior calculated in (B), what is the marginal posterior distribution $p(\omega | y)$? \\ 
- Unlike the previous question, where you could just read it off, here you have to integrate over $\theta$. Ignore constants not depending on $\omega$ in calculating this integral.
\begin{equation} \begin{split}
p(\omega | y) & = \int p(\theta,\omega | y) d\theta \\
& \propto \int \omega^{(d^*+1)/2 - 1}exp\{ -\omega \cdot \frac{\kappa^*(\theta - \mu^*)^2}{2}\}exp\{-\omega \cdot \frac{\eta^*}{2}\} d\theta \\
\end{split} \end{equation}

\end{enumerate}

\clearpage

\appendix
\chapter{R code}
\label{chap:code}




\end{document}
