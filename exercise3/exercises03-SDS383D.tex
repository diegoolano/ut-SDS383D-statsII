\documentclass{homework}

\linespread{1.2}

\usepackage[T1]{fontenc}\usepackage{palatino}
\usepackage{amssymb,amsmath,amsthm,mathtools}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[babel]{csquotes}
\usepackage[pdftex]{hyperref}
\usepackage{enumitem}
\usepackage[usenames,dvipsnames,table,xcdraw]{xcolor}

\usepackage{graphicx} 
\usepackage{verbatim} % Commenti in blocco con \begin{comment}
\usepackage{bm}
\usepackage[font={small,it}]{caption}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{array}
\usepackage{enumitem}
\setlist[enumerate]{label*=\arabic*.}
	
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{listings}

\definecolor{mygraybackground}{gray}{0.95}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

\lstset{language=C++,
backgroundcolor=\color{lbcolor},
    tabsize=3,    
        basicstyle=\scriptsize\ttfamily,
        showstringspaces=false,
        breaklines=true,
        frame=single,
        numbers=left,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color{blue},
        captionpos=b,   
        commentstyle=\color{ForestGreen},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        numberstyle=\small\ttfamily\color{Gray}
}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\newcommand{\tr}{\text{tr}}
\newcommand{\pN}{\mathcal{N}}
\newcommand{\R}{\textsf{R} }
\newcommand{\1}{\mathbf{1}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% Simbolo iid
\newcommand\iid{\stackrel{\mathclap{\normalfont\tiny\mbox{iid}}}{\sim}}
% Simbolo ind
\newcommand\ind{\stackrel{\mathclap{\normalfont\tiny\mbox{ind}}}{\sim}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\sign}{sign}



\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\title{SDS 383D: Exercise 3}
\author{Diego Garcia-Olano}

\begin{document}

\makeatletter
\begin{titlepage}
	\vspace*{\fill}
	\centering
	{\huge \@title \par}
	\vskip0.5cm
	{\large \@author \par}
	\vskip0.5cm
	{\large \today \par}
	\vspace*{\fill}
\end{titlepage}
\makeatother

\newpage 
\mbox{}
\thispagestyle{empty}

\setcounter{page}{1}


\section{Basic concepts}

\subsection{Bias--variance decomposition}

Let $\hat{f}(x)$ be a noisy estimate of some function $f(x)$, evaluated at some point $x$.  Define the mean-squared error of the estimate as
$$
MSE(\hat{f}, f) = E\{ [ f(x) - \hat{f}(x)]^2 \} , .
$$
Prove that $MSE(f, \hat{f}) = B^2 + v$, where
$$
B = E \{ \hat{f}(x) \} - f(x) \quad \mbox{and} \quad V = var\{ \hat{f}(x) \} \, .
$$

\subsection{A simple example  (optional problem)}

Some people refer to the above decomposition as the \textit{bias--variance tradeoff.}  Why a tradeoff?  Here's a simple example to convey the intuition.

Suppose we observe $x_1, \ldots, x_n$ from some distribution $F$, and want to estimate $f(0)$, the value of the probability density function at 0.  Let $h$ be a small positive number, called the \textit{bandwidth}, and define the quantity
$$
\pi_h = P\left( -\frac{h}{2} < X < \frac{h}{2} \right) = \int_{-h/2}^{h/2} f(x) dx \, .
$$
Clearly $\pi_h \approx h f(0)$.

\begin{enumerate}[label=(\Alph*)]
\item Let $Y$ be the number of observations in a sample of size $n$ that fall within the interval $(-h/2, h/2)$.  What is the distribution of $Y$?  What are its mean and variance in terms of $n$ and $\pi_h$?  Propose a simple estimator $\hat{f}(0)$ involving $Y$.
\item Suppose we expand $f(x)$ in a second-order Taylor series about $0$:
$$
f(x) \approx f(0) + x f'(0) + \frac{x^2}{2} f''(0) \, .
$$
Use this in the above expression for $\pi_h$, together with the bias--variance decomposition, to show that
$$
MSE\{ \hat{f}(0), f(0) \} \approx A h^4 + \frac{B}{nh}
$$
for constants $A$ and $B$ that you should (approximately) specify.  What happens to the bias and variance when you make $h$ small?  When you make $h$ big?

\item Use this result to derive an expression for the bandwidth that minimizes mean-squared error, as a function of $n$.  You can approximate any constants that appear, but make sure you get the right functional dependence on the sample size.

\end{enumerate}


%Use this to prove the following theorem.
%\begin{theorem}
%Let $x$ be fixed, and suppose that $f'(x)$ is absolutely continuous and that $\int (f'(u))^2 du < \infty$.
%\end{theorem}
%These steps will walk you through it.
%\begin{enumerate}
%\item Observe that, by Taylor's theorem, for any $x$ and $u$ in $B_j$,
%$$
%f(u) = f(x) + (u-x) f'(x) + \frac{(u-x)^2}{2} f''(\tilde{x})
%$$
%for some $\tilde{x}$ between $x$ and $u$.  Use this fact to show that
%$$
%p_j = h f(x) + h f'(x) \left( h(j-1/2) - x \right) + O(h^3) \, .
%$$
%\item From this, show that the bias of $\hat{f}(x)$ is
%$$
%
%$$
%\end{enumerate}


\section{Curve fitting by linear smoothing}

Consider a nonlinear regression problem with one predictor and one response: $y_i = f(x_i) + \epsilon_i$, where the $\epsilon_i$ are mean-zero random variables.

\begin{enumerate}[label=(\Alph*)]
\item Suppose we want to estimate the value of the regression function $y^{\star}$ at some new point $x^\star$, denoted $\hat{f}(x^{\star})$.  Assume for the moment that $f(x)$ is linear, and that $y$ and $x$ have already had their means subtracted, in which case $y_i = \beta x_i + \epsilon_i$.

Return to your least-squares estimator for multiple regression.  Show that for the one-predictor case, your prediction $\hat{y}^{\star} = f(x^{\star}) = \hat{\beta} x^{\star}$ may be expressed as a \textit{linear smoother}\footnote{This doesn't mean the weight function $w(\cdot)$ is linear in its arguments, but rather than the new prediction is a linear combination of the past outcomes $y_i$.} of the following form:
$$
\hat{f}(x^{\star}) =  \sum_{i=1}^n w(x_i, x^{\star}) y_i   \, 
$$
for any $x^{\star}$.  Inspect the weighting function you derived.  Briefly describe your understanding of how the resulting smoother behaves, compared with the smoother that arises from an alternate form of the weight function $w(x_i, x^{\star})$:
$$
w_K(x_i, x^{\star}) = \left \{
\begin{array}{l l}
1/K, & \mbox{$x_i$ one of the $K$ closest sample points to $x^{\star}$} \, ,\\
0, & \mbox{otherwise.} \\
\end{array}
\right.
$$
This is referred to as \textit{K-nearest-neighbor smoothing}.
\\
\par From prior results, we know: 
\begin{equation} \begin{split}
\hat{B} &= (X^TX)^{-1}X^T y\\
&= \frac{ X^Ty }{X^X} \\
&= \frac{ \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})}}{ \sum_{i=1}^n{(x_i - \bar{x})^2 }} \\
\end{split}\end{equation}

\par Using that we now show,
\begin{equation} \begin{split}
f(x^{\star}) &= \hat{\beta} x^{\star}\\
&= \frac{ \sum_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})}}{ \sum_{i=1}^n{(x_i - \bar{x})^2 }} \cdot x^{\star}\\
&= \frac{ \sum_{i=1}^n{x_i y_i }}{ \sum_{i=1}^n{x_i^2 }} \cdot x^{\star}\\
&= \frac{ \sum_{i=1}^n{x_i x^{\star} y_i }}{ \sum_{i=1}^n{x_i^2 }} \\
\end{split}\end{equation}

\par This equation is now in the form of our linear smoother
$$ \hat{f}(x^{\star}) =  \sum_{i=1}^n w(x_i, x^{\star}) y_i   \, $$

\par The linear smoother thus gives each $y_i$ a weight proportional to $x_i * x^{\star}$ and the sum of all x's.\\
where the K-nn one gives each $y_i$ a weight proportional to the K nearest $x_i$ to $x^{\star}.$
\\ the weight function for the linear smoother is influenced the most by bigger X, so those farthest from origin, and has nothing to do with how close they are to the x*
\\
\item A \textit{kernel function} $K(x)$ is a smooth function satisyfing
$$
\int_\mathbb{R} K(x) dx = 1 \; , \quad \int_\mathbb{R} x K(x) dx = 0 \; , \quad \int_\mathbb{R} x^2 K(x) dx > 0 \, .
$$
A very simple example is the uniform kernel,
$$
K(x) = \frac{1}{2} I(x) \quad \mbox{where} \quad I(x) = 
\left\{
\begin{array}{l l}
1, & |x| \leq 1 \\
0, & \mbox{otherwise} \, . \\
\end{array}
\right.
$$
Another common example is the Gaussian kernel:
$$
K(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2/2} \, .
$$

Kernels are used as weighting functions for taking local averages.  Specifically, define the weighting function
$$
w(x_i, x^{\star}) = \frac{1}{h} K \left( \frac{x_i - x^{\star}}{h} \right)  \, ,
$$
where $h$ is the bandwidth.    Using this weighting function in a linear smoother is called \textit{kernel regression}.  (The weighting function gives the unnormalized weights; you should normalize the weights so that they sum to 1.)

Write your own R function that will fit a kernel smoother for an arbitrary set of $x$-$y$ pairs, and arbitrary choice of (positive real) bandwidth $h$.\footnote{Coding tip: write things in a modular way.  A kernel function is a function accepting a distance and a bandwidth and returning a nonnegative real.  A weighting function is a function that accepts a vector of previous $x$'s, a new x, and a kernel function; and that returns a vector of weights.  Et cetera.  You will appreciate your foresight in writing modular code later in the course!}  Set up an R script that will simulate noisy data from some nonlinear function, $y = f(x) + \epsilon$; subtract the sample means from the simulated $x$ and $y$; and use your function to fit the kernel smoother for some choice of $h$.  Plot the estimated functions for a range of bandwidths large enough to yield noticeable changes in the qualitative behavior of the prediction functions.

\textbf{See "exercises3\_curvefitting\_b.R"}

\end{enumerate}

\section{Cross validation}

Left unanswered so far in our previous study of kernel regression is the question: how does one choose the bandwidth $h$ used for the kernel?  Assume for now that the goal is to predict well, not necessarily to recover the truth.  (These are related but distinct goals.)  

\begin{enumerate}[label=(\Alph*)]


\item  Presumably a good choice of $h$ would be one that led to smaller predictive errors on fresh data.  Write a function or script that will: (1) accept an old (``training'') data set and a new (``testing'') data set as inputs; (2) fit the kernel-regression estimator to the training data for specified choices of $h$; and (3) return the estimated functions and the realized prediction error on the testing data for each value of $h$.  This should involve a fairly straightforward ``wrapper'' of the function you've already written.

\textbf{See "exercise3/exercises3\_crossvalidation.R"}

\item Imagine a conceptual two-by-two table for the unknown, true state of affairs.  The rows of the table are ``wiggly function'' and ``smooth function,'' and the columns are ``highly noisy observations'' and ``not so noisy observations.''  Simulate one data set (say, 500 points) for each of the four cells of this table, where the $x$'s take values in the unit interval.  Then split each data set into training and testing subsets.  You choose the functions.\footnote{Trigonometric functions, for example, can be pretty wiggly if you make the period small.}   Apply your method to each case, using the testing data to select a bandwidth parameter.  Choose the estimate that minimizes the average squared error in prediction, which estimates the mean-squared error:
$$
L_n(\hat{f}) = \frac{1}{n}\sum_{i=1}^{n^{\star}} (y^{\star}_i - \hat{y}_i^{\star} )^2 \, ,
$$
where $(y_i^{\star}, x_i^{\star})$ are the points in the test set, and $ \hat{y}_i^{\star}$ is your predicted value arising from the model you fit using only the training data.  Does your out-of-sample predictive validation method lead to reasonable choices of $h$ for each case?

\textbf{See "exercise3/exercises3\_crossvalidation.R" and figures "curve\_fitting\_b2 and b3"}

\item (optional problem) Splitting a data set into two chunks to choose $h$ by out-of-sample validation has some drawbacks.  List at least two.  Then consider an alternative: leave-one-out cross validation.  Define
$$
\mbox{LOOCV} = \sum_{i=1}^n \left( y_i - \hat{y}_{i}^{(-i)} \right)^2 \, ,
$$
where $\hat{y}_{i}^{(-i)} $ is the predicted value of $y_i$ obtained by omitting the $i$th pair and fitting the model to the reduced data set.\footnote{The intuition here is straightforward: for each possible choice of $h$, you have to predict each data point using all the others.  The bandwidth that with the lowest prediction error is the ``best'' choice by the LOOCV criterion.}  This is contingent upon a particular bandwidth, and is obviously a function of $x_i$, but these dependencies are suppressed for notational ease.  This looks expensive to compute: for each value of $h$, and for each data point to be held out, fit a whole nonlinear regression model.  But you will derive a shortcut!

Observe that for a linear smoother, we can write the whole vector of fitted values as $\hat{y} = H y$, where $H$ is called the smoothing matrix (or ``hat matrix'') and $y$ is the vector of observed outcomes.\footnote{Remember that in multiple linear regression this is also true: $$\hat{y} = X \hat{\beta} = X (X^T X)^{-1} X^T y = Hy \, .$$}  Write $\hat{y}_i$ in terms of $H$ and $y$, and show that $\hat{y}_i^{(-i)} = \hat{y}_i - H_{ii} y_i + H_{ii} \hat{y}_i^{(-i)}$.  Deduce that, for a linear smoother,
$$
\mbox{LOOCV} = \sum_{i=1}^n \left( \frac{  y_i - \hat{y}_{i} } {1-H_{ii}} \right)^2 \, .
$$


\end{enumerate}



\section{Local polynomial regression}

Kernel regression has a nice interpretation as a ``locally constant'' estimator, obtained from locally weighted least squares.  To see this, suppose we observe pairs $(x_i, y_i)$ for $i = 1, \ldots, n$ from our new favorite model, $y_i = f(x_i) + \epsilon_i$ and wish to estimate the value of the underlying function $f(x)$ at some point $x$ by weighted least squares.  Our estimate is the scalar\footnote{Because we are only talking about the value of the function at a specific point $x$, not the whole function.} quantity
$$
\hat{f}(x) = a = \arg \min_{\mathbb{R}} \sum_{i=1}^n w_i (y_i - a)^2 \, ,
$$
where the $w_i$ are the normalized weights (i.e.~they have been rescaled to sum to 1 for fixed $x$).  Clearly if $w_i = 1/n$, the estimate is simply $\bar{y}$, the sample mean, which is the ``best'' globally constant estimator.  Using elementary calculus, it is easy to see that if the unnormalized weights are
$$
w_i \equiv w(x, x_i) = \frac{1}{h} K \left( \frac{x_i - x}{h} \right)  \, ,
$$
then the solution is exactly the kernel-regression estimator.

\begin{enumerate}[label=(\Alph*)]

\item  A natural generalization of locally constant regression is local polynomial regression.  For points $u$ in a neighborhood of the target point $x$, define the polynomial
$$
g_{x}(u; a) = a_0 + \sum_{k=1}^D a_j(u-x)^k 
$$
for some vector of coefficients $a = (a_0, \ldots, a_D)$.  As above, we will estimate the coefficients $a$ in $g_{x}(u; a)$ at some target point $x$ using weighted least squares:
$$
\hat{a} = \arg \min_{R^{D+1}} \sum_{i=1}^n w_i \left\{ y_i - g_{x}(x_i; a)  \right\}^2 \, ,
$$
where $w_i \equiv w(x_i, x)$ are the kernel weights defined just above, normalized to sum to one.\footnote{We are fitting a different polynomial function for every possible choice of $x$.  Thus $\hat{a}$ depends on the target point $x$, but we have suppressed this dependence for notational ease.} Derive a concise (matrix) form of the weight vector $\hat{a}$, and by extension, the local function estimate $\hat{f}(x)$ at the target value $x$.\footnote{Observe that at the target point $x$, $g_x(u = x; a) = a_0$.  That is, only the constant term appears.  But this is not the same thing as fitting only a constant term!}   Life will be easier if you define the matrix $R_x$ whose $(i,j)$ entry is $(x_i-x)^{j-1}$, and remember that (weighted) polynomial regression is the same thing as (weighted) linear regression with a polynomial basis.

Note: if you get bogged down doing the general polynomial case, just try the linear case.
\\
\par First, by pluging in the definition of $g_x(x_i;a)$ into $\hat{a}$ we get
\begin{equation} \begin{split}
\hat{a} &= \arg \min_{R^{D+1}} \sum_{i=1}^n w_i \left\{ y_i -  ( a_0 + \sum_{k=1}^D a_k(x_i-x)^k )   \right\}^2 \\
&= \arg \min_{R^{D+1}} \sum_{i=1}^n w_i \left\{ y_i -  ( \sum_{k=0}^D a_k(x_i-x)^k )   \right\}^2 \\
\end{split}\end{equation}
\par Now by substituting in $R_x$ for $(x_i - x)^k)$ and taking into account the definition for $R_x$ is based on $j-1$, we can  rewrite the following in matrix notation to get,
$$ \hat{a} = \arg \min_{R^{D+1}} ( y - Ra)^T W (y - Ra) $$
We minimize the function by taking the derivative with respect to a and solving for zero

\begin{equation} \begin{split}
0 &= \frac{d}{da}\left\{ ( y - Ra)^T W (y - Ra) \right\}  \\
0 &= \frac{d}{da}\left\{ ( y^T - (Ra)^T) (Wy - WRa) \right\} \\
0 &= \frac{d}{da}\left\{ ( y^TWy - (Ra)^TWy  - y^TWRa + (Ra)^TWRa ) \right\}  \\
0 &= \frac{d}{da}\left\{ ( y^TWy -2y^TWRa + a^TR^TWRa ) \right\}  \\
0 &= 0 -2y^TWR + 2a^T(R^TWR)   \\
0 &= - 2y^TWR + 2R^TWRa \\
y^TWR &= R^TWRa \\
R^TWy &= R^TWRa \\
(R^TWR)^{-1}R^TWy &=  \hat{a}\\
\end{split}\end{equation}
* Remember from matrix calculus, if $y = x^TAx$ then $\frac{d}{dx} = x^T(a + a^T)$ hence, 
\begin{equation} \begin{split}
\frac{d}{dx} [ (a^T)R^TWR(a) ] & = a^T(R^TWR + (R^TWR)^T) \\
&=a^T(R^TWR + (R^T)(R^TW)^T)\\
&=a^T(R^TWR + (R^T)(W^TR))\\
&=a^T(R^TWR + R^TW^TR)\\
&=a^T(R^T( WR + W^TR))\\
&=a^T(R^T( W + W^T)R)\\
\end{split}\end{equation}
W is a diagnol matrix of weights so $W = W^T$ and hence,
$$ a^T(R^T( W + W )R) = 2a^T(R^TWR)$$

\item From this, conclude that for the special case of the local linear estimator ($D=1$), we can write $\hat{f}(x)$ as a linear smoother of the form
$$
\hat{f}(x) = \frac{\sum_{i=1}^n w_i(x) y_i }{\sum_{i=1}^n w_i(x)} \, ,
$$
where the unnormalized weights are
\begin{eqnarray*}
w_i(x) &=& K \left( \frac{x-x_i}{h} \right) \left\{  s_2(x) - (x_i-x) s_1(x) \right\}\\
s_j(x) &=& \sum_{i=1}^n K \left( \frac{x-x_i}{h} \right) (x_i-x)^j \, .
\end{eqnarray*}

\par From before, $\hat{f_a} = \hat{a} = (R^TWR)^{-1}R^TWy$ , and  \\for D=1, $\hat{a} \in \mathbb{R}^{D+1} = \mathbf{R}^2$,\\
$R_x = \begin{bmatrix}
       1 & (x_1 - x)        \\[0.3em]
       \cdots & \cdots    \\
       1 & (x_n - x)        \\[0.3em]
     \end{bmatrix}$
$R_x^TW = \begin{bmatrix}
       w_1 & w_n       \\[0.3em]
       w_1(x_1 - x) & w_n(x_n - x)        \\[0.3em]
     \end{bmatrix}$
\\$R_x^TWR_x = \begin{bmatrix}
       \sum w_1 & \sum w_i(x_i - x)       \\[0.3em]
       \sum w_1(x_1 - x) & \sum w_i(x_i - x)^2        \\[0.3em]
     \end{bmatrix}$
\\
\par
$R_x^TWR_x = \begin{bmatrix}
      s_0(x) & s_1(x)     \\[0.3em]
      s_1(x) & s_2(x)        \\[0.3em]
     \end{bmatrix}$
\\
\par
$(R_x^TWR_x)^{-1} = \frac{1}{s_0s_2 - s_1^2} 
\begin{bmatrix}  s_2(x) & -s_1(x) \\[0.3em]   -s_1(x) & s_0(x)  \\[0.3em]   \end{bmatrix}$

So now, we have $\hat{a} =  \left( \frac{1}{s_0s_2 - s_1^2}  
\begin{bmatrix}  s_2(x) & -s_1(x) \\[0.3em]   -s_1(x) & s_0(x)  \\[0.3em]   \end{bmatrix} \right) 
\left( \begin{bmatrix}  \sum w_iy_i \\[0.3em]  \sum w_i(x_i - x)y_i  \\[0.3em]  \end{bmatrix} \right)$

\par $\hat{a_0} = \frac{1}{s_0s_2 - s_1^2}\left[  s_2(x)\sum w_iy_i  - s_1(x)\sum w_i(x_i - x)y_i \right]$
\par $\hat{a_0} = \frac{1}{s_0s_2 - s_1^2}\left(  \underbrace{ \sum w_i \big[ s_2(x)  - (x_i - x) s_1(x)} \big] y_i \right)$ 
     
\par where  $\frac{1}{s_0s_2 - s_1^2}  = \sum w_is_2 - \sum w(x_i - x)s_1 =  \underbrace{ \sum w_i \big[ s_2(x) - (x_i - x) s_1(x) \big]}$

\par which puts $\hat{f}(x)$ in the form of the linear smoother from above .



\item Suppose that the residuals have constant variance $\sigma^2$ (that is, the spread of the residuals does not depend on $x$).  Derive the mean and variance of the sampling distribution for the local polynomial estimate $\hat{f}(x)$ at some arbitrary point $x$.  Note: the random variable $\hat{f}(x)$ is just a scalar quantity at $x$, not the whole function.



\item We don't know the residual variance, but we can estimate it.  A basic fact is that if $x$ is a random vector with mean $\mu$ and covariance matrix $\Sigma$, then for any symmetric matrix $Q$ of appropriate dimension, the quadratic form $x^T Q x$ has expectation
$$
E(x^T Q x) = \mbox{tr}(Q \Sigma) + \mu^T Q \mu \, .
$$
Write the vector of residuals as $r = y - \hat{y} = y - Hy$, where $H$ is the smoothing matrix.  Compute the expected value of the estimator
$$
\hat{\sigma}^2 = \frac{\Vert r \Vert_2^2}{n - 2\mbox{tr}(H) + \mbox{tr}(H^T H)} \, ,
$$
and simplify things as much as possible.  Roughly under what circumstances will this estimator be nearly unbiased for large $n$?  Note: the quantity $2\mbox{tr}(H) - \mbox{tr}(H^T H)$ is often referred to as the ``effective degrees of freedom'' in such problems.

\par By assumption, $E(\mathbf{y}) = f(\mathbf{x})$ and $\text{cov}(\mathbf{y}) = \sigma^2 I$ then, 	
\begin{equation} \begin{split}
	E \left( \Vert r \Vert_2^2 \right) &= E \left( (\y - H\y)^T(\y-H\y) \right) \\
	&= E \left( \y^T \y - 2\y^T H \y + \y^T H^T H \y \right) \\
	&= E\left(\y^T \y  \right) - 2 E \left( \y^T H \y \right) + E \left( \y^T H^T H \y \right) \\
	&= \left( \tr\left[ I\sigma^2 I \right] + f(\x)^T f(\x) \right) - 2 \left( \tr\left[ H^T\sigma^2 I \right] + f(\x)^T H^T f(\x) \right) + \left( \tr \left[ H^T H \sigma^2 I \right] + f(\x)^T H^T H f(\x)\right) \\
	&= \left( n\sigma^2 + f(\x)^T f(\x) \right) - 2 \left( \sigma^2 \tr[H] + f(\x)^T H^T f(\x) \right) + \left( \sigma^2 \tr[H^TH] + f(\x)^T H^T H f(\x)\right) \\
	&= \left(n - \tr[H] + \tr [H^T H] \right) \sigma^2 + \left( f(\x)^T f(\x) - 2 f(\x)^T H^T f(\x) + f(\x)^T H^T H f(\x) \right) \\
	&= \left(n - \tr[H] + \tr [H^T H] \right) \sigma^2 + \left( f(\x) - Hf(\x) \right)^T \left( f(\x) - Hf(\x) \right),\\
\end{split} \end{equation}
	so the estimator $$\hat{\sigma}^2 = \frac{\| r^2 \|_2^2}{n - \tr[H] + \tr [H^T H]}$$ will be nearly unbiased in $\sigma^2$ when $f(\x) \approx H f(\x)$.


\item Write a new R function that fits the local linear estimator using a Gaussian kernel for a specified choice of bandwidth $h$. Then load the data in ``utilities.csv'' into R.\footnote{On the class GitHub site.}  This data set shows the monthly gas bill (in dollars) for a single-family home in Minnesota, along with the average temperature in that month (in degrees F), and the number of billing days in that month.  Let $y$ be the average daily gas bill in a given month (i.e.~dollars divided by billing days), and let $x$ be the average temperature.  Fit $y$ versus $x$ using local linear regression and some choice of kernel.  Choose a bandwidth by leave-one-out cross-validation.

\item Inspect the residuals from the model you just fit.  Does the assumption of constant variance (homoscedasticity) look reasonable?  If not, do you have any suggestion for fixing it?

\item Put everything together to construct an approximate point-wise 95\% confidence interval for the local linear model (using your chosen bandwidth) for the value of the function at each of the observed points $x_i$ for the utilities data.  Plot these confidence bands, along with the estimated function, on top of a scatter plot of the data.\footnote{It's fine to use Gaussian critical values for your confidence set.}

\end{enumerate}


\section{Gaussian processes}

A \textit{Gaussian process} is a collection of random variables $\{f(x): x \in \mathcal{X}\}$ such that, for any finite collection of indices $x_1, \ldots, x_N \in \mathcal{X}$, the random vector $[f(x_1), \ldots, f(x_N)]^T$ has a multivariate normal distribution.  It is a generalization of the multivariate normal distribution to infinite-dimensional spaces. The set $\mathcal{X}$ is called the index set or the state space of the process, and need not be countable.

A Gaussian process can be thought of as a random function defined over $\mathcal{X}$, often the real line or $\mathbb{R}^p$.  We write $f \sim \mbox{GP}(m, C)$ for some mean function $m: \mathcal{X} \rightarrow \mathbb{R}$ and a covariance function $C: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}^+$.  These functions define the moments\footnote{And therefore the entire distribution, because it is normal} of all finite-dimensional marginals of the process, in the sense that
$$
E\{ f(x_1) \} = m(x_1) \quad \mbox{and} \quad \mbox{cov}\{f(x_1), f(x_2) \} = C(x_1, x_2)
$$
for all $x_1, x_2 \in \mathcal{X}$.  More generally, the random vector $[f(x_1), \ldots, f(x_N)]^T$ has covariance matrix whose $(i,j)$ element is $C(x_i, x_j)$.  Typical covariance functions are those that decay as a function of increasing distance between points $x_1$ and $x_2$.  The notion is that $f(x_1)$ and $f(x_2)$ will have high covariance when $x_1$ and $x_2$ are close to each other.

\begin{enumerate}[label=(\Alph*)]
\item   Read up on the \href{https://en.wikipedia.org/wiki/Mat\%C3\%A9rn_covariance_function}{Matern class}\footnote{\url{https://en.wikipedia.org/wiki/Mat\%C3\%A9rn_covariance_function}} of covariance functions.  The Matern class has the \textit{squared exponential} covariance function as a special case:
$$
C_{SE}(x_1, x_2) = \tau_1^2 \exp \left\{ - \frac{1}{2} \left( \frac{d(x_1, x_2)}{b} \right)^2 \right\} + \tau^2_2 \delta(x_1, x_2) \, ,
$$
where $d(x_1, x_2) = \Vert x_1 - x_2 \Vert_2$ is Euclidean distance (or just $|x-y|$ for scalars).  The constants $(b, \tau^2_1, \tau^2_2)$ are often called \textit{hyperparameters}, and $\delta(a,b)$ is the Kronecker delta function that takes the value 1 if $a=b$, and 0 otherwise.  But usually this covariance function generates functions that are ``too smooth,'' and so we use other covariance functions in the Matern class as a default.\footnote{See the speed comparison in kernel-benchmark.R on the class GitHub site if you want to see how Rcpp can be used to speed things up here.  My code is for the squared-exponential covariance function.}

Let's start with the simple case where $\mathcal{X} = [0,1]$, the unit interval.  Write a function that simulates a mean-zero Gaussian process on $[0,1]$ under the Matern(5/2) covariance function.  The function will accept as arguments: (1) finite set of points $x_1, \ldots, x_N$ on the unit interval; and (2) a triplet $(b, \tau^2_1, \tau^2_2)$. It will return the value of the random process at each point: $f(x_1), \ldots, f(x_N)$.

Use your function to simulate (and plot) Gaussian processes across a range of values for $b$, $\tau^2_1$, and $\tau^2_2$.  Try starting with a very small value of $\tau^2_2$ (say, $10^{-6}$) and playing around with the other two first.  On the basis of your experiments, describe the role of these three hyperparameters in controlling the overall behavior of the random functions that result.  What happens when you try $\tau^2_2 = 0$? Why?  If you can fix this, do---remember our earlier discussion on different ways to simulate the MVN.

Now simulating a few functions with a different covariance function, the Mat\'ern with parameter $5/2$:
$$
C_{M52}(x_1, x_2) = \tau_1^2 \left\{ 1 + \frac{\sqrt{5}d}{b} + \frac{5d^2}{3b^2} \right\} \exp\left( \frac{-\sqrt{5}d}{b} \right) + \tau^2_2 \delta(x_1, x_2) \, ,
$$
where $d = \Vert x_1 - x_2 \Vert_2$ is the distance between the two points $x_1$ and $x_2$.  Comment on the differences between the functions generated from the two covariance kernels.\footnote{The Mat\'ern covariance is actually a whole family of functions: \url{https://en.wikipedia.org/wiki/Mat\%C3\%A9rn_covariance_function}.}

\item Suppose you observe the value of a Gaussian process $f \sim \mbox{GP}(m,C)$ at points $x_1, \ldots, x_N$.  What is the conditional distribution of the value of the process at some new point $x^{\star}$?  For the sake of notational ease simply write the value of the $(i,j)$ element of the covariance matrix as $C_{i,j}$, rather than expanding it in terms of a specific covariance function.


Let  $p(f (x^{*})|x^{*} , \textbf{x}, f(x))$ be the predictive distribution for the value of the gaussian process at a new point.  By definition of Gaussian process,\\
$  \begin{bmatrix} f(x^*) \\  f(x_1)   \\ \vdots \\ f(x_N) \end{bmatrix}
\sim \left( \begin{bmatrix} m(x^*) \\ m(x_1) \\ \vdots \\ m(x_N) \end{bmatrix}
; \begin{bmatrix} 
C(x^*,x^*) & C(x_1,x^*) & \cdots & C(x_N,x^*) \\ 
C(x^*,x_1) & C(x_1,x_1) & \cdots & C(x_N,x_1) \\ 
\vdots & \vdots & \ddots & \vdots \\
C(x^*,x_N) & C(x_1,x_N) & \cdots & C(x_N,x_N) \\ 
\end{bmatrix} \right)$\\

Since we are in a multivariate setting, we can now use Homework 1 results about conditional distributions of the multivariate Normal distributions and write$$f(x^*) | f(x_1 ), \cdots , f(x_N ) \sim N (m(x^*) + \Sigma_{12} \Sigma_{22}^{-1} [f(x) - m(x)]; C(x^*, x^*) - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} ) $$where: \\ 
$\Sigma_{12} = (C(x^*,x_1),...,C(x^*,x_N))$\\$ \Sigma_{22} = \begin{bmatrix} C(x_1,x_1) & \cdots & C(x_1,x_N) \\\vdots & \ddots & \vdots \\C(x_N,x_1) & \cdots & C(x_N,x_N)\\
\end{bmatrix}$



\item Prove the following lemma.

%\begin{lemma}
Suppose that the joint distribution of two vectors $y$ and $\theta$ has the following properties: \\ (1) the conditional distribution for $y$ given $\theta$ is multivariate normal, $(y \mid \theta) \sim N(R\theta, \Sigma)$; and \\(2) the marginal distribution of $\theta$ is multivariate normal, $\theta \sim N(m,V)$.  \\Assume that $R$, $\Sigma$, $m$, and $V$ are all constants.  \\Then the joint distribution of $y$ and $\theta$ is multivariate normal.
%\end{lemma}
\\
\par Affine approach: \\
$y = R\theta + \epsilon $\\
$\epsilon \sim N(0, \Sigma)$\\
$\theta \sim N(m,v)$\\
$\begin{bmatrix} y \\ \theta \end{bmatrix} = \begin{bmatrix} R \\ I \end{bmatrix} \theta +   \begin{bmatrix} I \\ O \end{bmatrix} \epsilon $

\end{enumerate}


\section{In nonparametric regression and spatial smoothing}

\begin{enumerate}[label=(\Alph*)]

\item Suppose we observe data $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim N(0, \sigma^2)$, for some unknown function $f$. \\Suppose that the prior distribution for the unknown function is\\ a mean-zero Gaussian process: $f \sim \mbox{GP}(0, C)$ for some covariance function $C$.  \\Let $x_1, \ldots, x_N$ denote the previously observed $x$ points.  \\Derive the posterior distribution for the random vector $[f(x_1), \ldots, f(x_N)]^T$, \\given the corresponding outcomes $y_1, \ldots, y_N$, assuming that you know $\sigma^2$.
\\
\par If we observe $\{(x_1,y_1);\cdots;(x_n,y_n)\}$ where $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim N(0,\sigma_2)$ \\and $f$ has a Gaussian process prior, we have the following model:\\$\textbf{Y} | \textbf{f(x)}, \textbf{x}, \sigma^2 \sim N_n(\textbf{f(x)}; \sigma^2 I)$ \\$\textbf{f} \sim GP(0;C)  \rightarrow \textbf{f(x)} \sim N_n(\textbf{0},C(\textbf{x},\textbf{x}))$\\
\par Now we can find the posterior for the random vector $(f(x_1 ), . . . , f(x_n ))$ denoted  $\textbf{f}$\\

\begin{equation} \begin{split}
p(\textbf{f} | \textbf{y}, \textbf{x}, \sigma^2) &\propto p(\textbf{y} | \textbf{f}, \textbf{x}, \sigma^2) * p(\textbf{f}|\textbf{x}, \sigma^2) \\& = (\frac{1}{2\pi \sigma^2})^{\frac{n}{2}} exp\left( -\frac{1}{2}( \textbf{y} - \textbf{f})^T \frac{I}{\sigma^2}(\textbf{y} - \textbf{f}) \right) \\
& \times  (\frac{1}{2\pi})^{\frac{n}{2}} | C |^{\frac{-1}{2}} exp\left( -\frac{1}{2}\textbf{f}^TC^{-1} \textbf{f}\right)\\
&\propto exp\left( -\frac{1}{2}( \textbf{y} - \textbf{f})^T \frac{I}{\sigma^2}(\textbf{y} - \textbf{f}) + \textbf{f}^TC^{-1} \textbf{f} \right)\end{split} \end{equation}
We can now complete the square within the $exp$
\begin{equation} \begin{split}
&= -\frac{1}{2}( \textbf{y} - \textbf{f})^T \frac{I}{\sigma^2}(\textbf{y} - \textbf{f}) + \textbf{f}^TC^{-1} \textbf{f}\\
&= -\frac{1}{2}( \textbf{y}^T - \textbf{f}^T) (\frac{I}{\sigma^2}\textbf{y} - \frac{I}{\sigma^2}\textbf{f}) + \textbf{f}^TC^{-1} \textbf{f}\\
&= -\frac{1}{2}( \textbf{y}^T\frac{I}{\sigma^2}\textbf{y}  - \textbf{f}^T\frac{I}{\sigma^2}\textbf{y} - \textbf{y}^T\frac{I}{\sigma^2}\textbf{f} + \textbf{f}^T\frac{I}{\sigma^2}\textbf{f}) + \textbf{f}^TC^{-1} \textbf{f}\\
&= -\frac{1}{2}( \textbf{y}^T\frac{I}{\sigma^2}\textbf{y}  - 2 \textbf{y}^T\frac{I}{\sigma^2}\textbf{f} + \textbf{f}^T\frac{I}{\sigma^2}\textbf{f}) + \textbf{f}^TC^{-1} \textbf{f}\\
&= -\frac{1}{2}\textbf{y}^T\frac{I}{\sigma^2}\textbf{y}  - \frac{1}{2}\textbf{f}^T\frac{I}{\sigma^2}\textbf{f} + \textbf{y}^T\frac{I}{\sigma^2}\textbf{f} +  \textbf{f}^TC^{-1} \textbf{f}\\
&= \frac{1}{2}\textbf{f}^T[ \frac{I}{\sigma^2} + C^{-1}]\textbf{f}  - \textbf{y}^T\frac{I}{\sigma^2}\textbf{f}  + \textbf{y}^T\frac{I}{\sigma^2}\textbf{f}\\
&\propto \frac{1}{2}[ \textbf{f} - \textbf{m}^* ]^T * C^{* - 1}(\textbf{f} - \textbf{m}^*)
\end{split} \end{equation}
where\\
$C^{* - 1} = C^{-1} + \frac{I}{\sigma^2} $\\
$ \textbf{m}^* = C^{*} \frac{I}{\sigma^2} \textbf{y}$\\
Therefore, $f(x_1),\cdots,f(x_n)| y , x ,\sigma^2 \sim N(m^*,C^*)$. \\
As we have seen, the precisions (inverse variance) add, and \\the posterior mean is the precision-weighted average of the prior mean and data.


\item As before, suppose we observe $y_i = f(x_i) + \epsilon_i$, $\epsilon_i \sim N(0, \sigma^2)$, for $i=1, \ldots, N$.  \\Now we wish to predict the value of the function $f(x^{\star})$ at some new point $x^{\star}$ \\where we haven't seen previous data.  \\Suppose that $f$ has a mean-zero Gaussian process prior, $f \sim GP(0, C)$.  \\Show that the posterior mean $E\{ f(x^{\star}) \mid y_1, \ldots, y_N \}$ is a linear smoother, and \\derive expressions both for the smoothing weights and the posterior variance of $f(x^{\star})$.
\\
\par This is similar to Part B of the previous Gaussian Processes section, except now we are observing noisy
$y_1\cdots y_n$ observations instead of denoised function values $f(x_1) \cdots f(x_n)$. 
\\We can take a similar approach to derive the posterior $f(x^*)|y$ for some new point $x^*$ we wish to predict.
We know that $y_i = f(x_i) + e_i$ , and $f \sim GP(0, C)$, and $e \sim N(0, \sigma^2 I)$.\\
\par The sum of multivariate gaussians is multivariate gaussian, so $\textbf{y} \sim N(0, C + \sigma^2I)$\\
We now use a similar technique as Gaussian Procceses Part B to construct the joint (partitioned) distribution
of $(f^*, y)$.\\
$  \begin{bmatrix} y \\ f(x^*)  \end{bmatrix}
\sim N \left(  \textbf{0} 
; \begin{bmatrix} 
C(x,x) + \sigma^2I & C(x*,x)^T \\ 
C(x*,x)  & C(x*,x*) \\ 
\end{bmatrix} \right)$\\
\par The only difference from the noise-free (Gaussian Processes, Part B) case is that the covariance matrix of the y's now has an extra $\sigma^2$ term added to diagonal. We can again use the multivariate conditional theory from exercise 1 to obtain the conditional $f^*|y$.
So now, \\
\begin{equation} \begin{split}
 f^* | y & \sim N( E[f* | y], Var[f^*|y]) \\
 E[f^* | y] & = C(x^*, \textbf{x}) (C(\textbf{x},\textbf{x}) + \sigma^2I)^{-1} y \\
 Var([f^*|y]) &= C(x^*, x^*) - C(x^*, x)^T(C(\textbf{x},\textbf{x}) + \sigma^2I)^{-1}C(x^*, x) \\
\end{split} \end{equation}
\\Thus we can write the mean as a linear smoother as follows:
$$E[ f* | y, x, x*, \sigma^2 ] = \Sigma_{i=1}^{n} w_i y_i $$
with $\textbf{w} = C(x^*, \textbf{x}) (C(\textbf{x},\textbf{x}) + \sigma^2I)^{-1} $

\item Go back to the utilities data, and plot the pointwise posterior mean and 95\% posterior confidence interval for the value of the function at each of the observed points $x_i$ (again, superimposed on top of the scatter plot of the data itself).  Choose $\tau^2_2$ to be very small, say $10^{-6}$, and choose $(b, \tau^2_1)$ that give a sensible-looking answer.\footnote{If you're bored with the utilities data, instead try the data in ethanol.csv, in which the NOx emissions of an ethanol engine are measured as the engine's fuel-air equivalence ratio (E in the data set) is varied.  Your goal would be to model NOx as a function of E using a Gaussian process. }

\item Let $y_i = f(x_i) + \epsilon_i$, and suppose that $f$ has a Gaussian-process prior under the Matern(5/2) covariance function $C$ with scale $\tau^1_2$, range $b$, and nugget $\tau^2_2$.  Derive an expression for the marginal distribution of $y = (y_1 \ldots, y_N)$ in terms of $(\tau^2_1, b, \tau^2_2)$, integrating out the random function $f$.  This is called a marginal likelihood.

\item Return to the utilities or ethanol data sets. Fix $\tau^2_2 = 0$, and evaluate the log of the marginal likelihood function $p(y \mid \tau^2_1, b)$ over a discrete 2-d grid of points.\footnote{Don't just use a black-box optimizer; we want to make sure we get the best solution if there are multiple modes.} If you're getting errors in your code with $\tau^2_2 = 0$, use something very small instead.  Use this plot to choose a set of values $(\hat{\tau^2_1}, \hat{b})$ for the hyperparameters.  Then use these hyperparameters to compute the posterior mean for $f$, given $y$.  Comment on any lingering concerns you have with your fitted model.

\item In \verb|weather.csv| you will find data on two variables from 147 weather stations in the American Pacific northwest.
\begin{itemize}
\item[pressure]: the difference between the forecasted pressure and the actual pressure reading at that station (in Pascals)
\item[temperature]: the difference between the forecasted temperature and the actual temperature reading at that station (in Celsius)
\end{itemize}
There are also latitude and longitude coordinates of each  station.  Fit a Gaussian process model for each of the temperature and pressure variables.  Choose hyperparameters appropriately.  Visualize your fitted functions (both the posterior mean and posterior standard deviation) on a regular grid using something like a contour plot or color image.  Read up on the \verb|image|, \verb|filled.contour|, or \verb|contourplot|\footnote{in the lattice library} functions in R.  An important consideration: is Euclidean distance the appropriate measure to go into the covariance function?  Or do we need separate length scales for the two dimensions, i.e.
$$
d^2(x, z) = \frac{(x_1 - z_1)^2}{b_1^2} +  \frac{(x_2 - z_2)^2}{b_2^2} \, .
$$
Justify your reasoning for using Euclidean distance or this ``nonisotropic'' distance.


%\item In \verb|droslong.csv|, you will find a small subset of a time-course DNA microarray experiment.  The gene-expression profiles of 2000 different genes in the fruit fly (Drosophila) genome are tracked over time during embryogenesis; you are getting data on 14 of these genes, organized in three groups (think of these as marking which cellular pathway that gene influences).  For each gene at each time point, there are 3 ``technical replicates''---that is, three copies of the same biological material from the same fly, run through the same process to measure gene expression.
%
%The question of interest is: how does each gene's expression profile change over time, as the process of embryogenesis unfolds?  Propose a hierarchical model for this data that properly reflects its structure.
%
%A nice graphics package is the ``lattice'' library.  Install and load this; then try commands such as
%\begin{verbatim}
%xyplot(log2exp~time | gene, data=droslong)
%xyplot(log2exp~time | group, data=droslong)
%\end{verbatim}
%to begin exploring the structure of this data.

\end{enumerate}



\end{document}

